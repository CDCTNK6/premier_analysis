{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras as keras\n",
    "import kerastuner\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tools import keras as tk\n",
    "import tools.preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # GLOBALS   \n",
    "DAY_ONE_ONLY = True\n",
    "TIME_SEQ = 225\n",
    "TARGET = \"multi_class\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "MAX_TRIALS = 500\n",
    "TEST_SPLIT = 0.2\n",
    "VAL_SPLIT = 0.1\n",
    "RAND = 2021\n",
    "TB_UPDATE_FREQ = 200\n",
    "WEIGHTED_LOSS = False\n",
    "\n",
    "# Paths\n",
    "pwd = globals()['_dh'][0]\n",
    "\n",
    "output_dir = os.path.abspath(os.path.join(pwd, \"..\", \"output\"))\n",
    "data_dir = os.path.abspath(os.path.join(pwd, \"..\", \"data\", \"data\"))\n",
    "tensorboard_dir = os.path.abspath(\n",
    "    os.path.join(data_dir, \"..\", \"model_checkpoints\"))\n",
    "pkl_dir = os.path.join(output_dir, \"pkl\")\n",
    "stats_dir = os.path.join(output_dir, \"analysis\")\n",
    "\n",
    "# Create analysis dir if it doesn't exist\n",
    "os.makedirs(stats_dir, exist_ok=True)\n",
    "\n",
    "stats_filename = TARGET + \"_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "with open(os.path.join(pkl_dir, TARGET + \"_trimmed_seqs.pkl\"), \"rb\") as f:\n",
    "    inputs = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
    "    vocab = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"feature_lookup.pkl\"), \"rb\") as f:\n",
    "    all_feats = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
    "    demog_lookup = pkl.load(f)\n",
    "\n",
    "# Determining number of vocab entries\n",
    "N_VOCAB = len(vocab) + 1\n",
    "N_DEMOG = len(demog_lookup) + 1\n",
    "MAX_DEMOG = max(len(x) for _, x, _ in inputs)\n",
    "N_CLASS = max(x for _, _, x in inputs) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics and callbacks\n",
    "callbacks = [\n",
    "    TensorBoard(\n",
    "    log_dir=os.path.join(tensorboard_dir, \"dan_hp_tune_tb\", \"\"),\n",
    "    histogram_freq=1,\n",
    "    profile_batch=0,\n",
    "    write_graph=False,\n",
    "    update_freq=TB_UPDATE_FREQ\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                min_delta=0,\n",
    "                                patience=3,\n",
    "                                restore_best_weights=True,\n",
    "                                mode=\"min\")\n",
    "]\n",
    "\n",
    "# Create some metrics\n",
    "metrics = [\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), name=\"ROC-AUC\"),\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), curve=\"PR\", name=\"PR-AUC\"),\n",
    "    # NOTE: I think F1 Score is kind of wonky here, but I'll add it anyways\n",
    "    # because that's pretty cool.\n",
    "    tfa.metrics.F1Score(num_classes=N_CLASS, average=\"weighted\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTV\n",
    "# Splitting the data\n",
    "train, test = train_test_split(\n",
    "    range(len(inputs)),\n",
    "    test_size=TEST_SPLIT,\n",
    "    stratify=[labs for _, _, labs in inputs],\n",
    "    random_state=RAND)\n",
    "\n",
    "train, validation = train_test_split(\n",
    "    train,\n",
    "    test_size=VAL_SPLIT,\n",
    "    stratify=[samp[2] for i, samp in enumerate(inputs) if i in train],\n",
    "    random_state=RAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DAY_ONE_ONLY:\n",
    "    # Optionally limiting the features to only those from the first day\n",
    "    # of the actual COVID visit\n",
    "    features = [l[0][-1] for l in inputs]\n",
    "else:\n",
    "    features = [tp.flatten(l[0]) for l in inputs]\n",
    "\n",
    "new_demog = [[i + N_VOCAB - 1 for i in l[1]] for l in inputs]\n",
    "features = [\n",
    "    features[i] + new_demog[i] for i in range(len(features))\n",
    "]\n",
    "demog_vocab = {k: v + N_VOCAB - 1 for k, v in demog_lookup.items()}\n",
    "vocab.update(demog_vocab)\n",
    "N_VOCAB = np.max([np.max(l) for l in features]) + 1\n",
    "\n",
    "# Making the variables\n",
    "X = keras.preprocessing.sequence.pad_sequences(features, padding='post')\n",
    "y = np.array([l[2] for l in inputs])\n",
    "\n",
    "N_FEATS = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique([labs for _, _, labs in inputs]).tolist()\n",
    "\n",
    "if WEIGHTED_LOSS:\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=classes,\n",
    "        y=[labs for _, _, labs in inputs],\n",
    "    )\n",
    "\n",
    "    class_weights = dict(zip(classes, class_weights))\n",
    "\n",
    "    print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project C:\\Users\\oet5\\premier_analysis\\data\\model_checkpoints\\dan_hp_tune\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from C:\\Users\\oet5\\premier_analysis\\data\\model_checkpoints\\dan_hp_tune\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hyper_model = tk.DANHyper(\n",
    "    vocab_size = N_VOCAB,\n",
    "    input_size=N_FEATS,\n",
    "    metrics = metrics,\n",
    "    n_classes = N_CLASS\n",
    ")\n",
    "\n",
    "tuner = kerastuner.tuners.BayesianOptimization(\n",
    "    hyper_model,\n",
    "    max_trials=MAX_TRIALS,\n",
    "    objective=\"val_loss\",\n",
    "    project_name=\"dan_hp_tune\",\n",
    "    # NOTE: This could be in output as well if we don't want to track/version it\n",
    "    directory=tensorboard_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "Feature Embedding L1 (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\n",
      "Feature Embedding L2 (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\n",
      "Embedding Dimension (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 64, 'max_value': 512, 'step': 64, 'sampling': None}\n",
      "Dropout from Embeddings (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.05, 'sampling': None}\n",
      "Dense Units (Int)\n",
      "{'default': 32, 'conditions': [], 'min_value': 2, 'max_value': 128, 'step': 1, 'sampling': 'log'}\n",
      "Learning Rate (Choice)\n",
      "{'default': 1e-06, 'conditions': [], 'values': [1e-06, 5e-06, 1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1], 'ordered': True}\n",
      "Momentum (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "# Announce the search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "Feature Embeddi...|0.01              |1e-06             \n",
      "Feature Embeddi...|0.01              |0                 \n",
      "Embedding Dimen...|384               |64                \n",
      "Dropout from Em...|0.65              |0.85              \n",
      "Dense Units       |70                |94                \n",
      "Learning Rate     |0.001             |0.005             \n",
      "Momentum          |0.4               |0.5               \n",
      "\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_1/kernel:0', 'dense_1/bias:0', 'Output/kernel:0', 'Output/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_1/kernel:0', 'dense_1/bias:0', 'Output/kernel:0', 'Output/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_1/kernel:0', 'dense_1/bias:0', 'Output/kernel:0', 'Output/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_1/kernel:0', 'dense_1/bias:0', 'Output/kernel:0', 'Output/bias:0'] when minimizing the loss.\n",
      "319/319 [==============================] - 18s 56ms/step - loss: 1326.5731 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 1170.2540 - val_ROC-AUC: 0.2569 - val_PR-AUC: 0.2393 - val_f1_score: 0.0692\n",
      "Epoch 2/15\n",
      "319/319 [==============================] - 15s 47ms/step - loss: 1027.9550 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 891.2846 - val_ROC-AUC: 0.2568 - val_PR-AUC: 0.2318 - val_f1_score: 0.0978\n",
      "Epoch 3/15\n",
      "319/319 [==============================] - 15s 47ms/step - loss: 768.3846 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 651.1772 - val_ROC-AUC: 0.4598 - val_PR-AUC: 0.2936 - val_f1_score: 0.1938\n",
      "Epoch 4/15\n",
      "319/319 [==============================] - 15s 46ms/step - loss: 547.4805 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 449.5031 - val_ROC-AUC: 0.3165 - val_PR-AUC: 0.2540 - val_f1_score: 0.0705\n",
      "Epoch 5/15\n",
      "319/319 [==============================] - 15s 46ms/step - loss: 364.7166 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 285.6823 - val_ROC-AUC: 0.2611 - val_PR-AUC: 0.2523 - val_f1_score: 0.0622\n",
      "Epoch 6/15\n",
      "319/319 [==============================] - 15s 47ms/step - loss: 219.5686 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 159.2591 - val_ROC-AUC: 0.2628 - val_PR-AUC: 0.2546 - val_f1_score: 0.0640\n",
      "Epoch 7/15\n",
      "319/319 [==============================] - 15s 46ms/step - loss: 111.6231 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 69.8412 - val_ROC-AUC: 0.2630 - val_PR-AUC: 0.2558 - val_f1_score: 0.0622\n",
      "Epoch 8/15\n",
      "319/319 [==============================] - ETA: 0s - loss: 40.4686 - ROC-AUC: 0.0000e+00 - PR-AUC: 0.0000e+00 - f1_score: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "if N_CLASS > 2:\n",
    "    # We have to pass one-hot labels for model fit, but CLF metrics\n",
    "    # will take indices\n",
    "    y_one_hot = np.eye(N_CLASS)[y]\n",
    "\n",
    "    tuner.search(X[train],\n",
    "                validation_data=(X[validation], y_one_hot[validation]),\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=callbacks\n",
    "    )\n",
    "else:\n",
    "    tuner.search(X[train],\n",
    "            validation_data=(X[validation], y_one_hot[validation]),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(os.path.join(tensorboard_dir, \"dan_hp_tune\", \"best\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
