{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import os\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import cond_rnn\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import kerastuner\n",
    "from kerastuner import HyperModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import tools.analysis as ta\n",
    "from tools import keras as tk\n",
    "import tools.preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # GLOBALS   \n",
    "TIME_SEQ = 225\n",
    "TARGET = \"multi_class\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "TEST_SPLIT = 0.2\n",
    "VAL_SPLIT = 0.1\n",
    "RAND = 2021\n",
    "TB_UPDATE_FREQ = 200\n",
    "\n",
    "# Paths\n",
    "output_dir = os.path.abspath(os.path.join(\"..\", \"output\"))\n",
    "data_dir = os.path.abspath(os.path.join(\"..\", \"data\", \"data\"))\n",
    "tensorboard_dir = os.path.abspath(\n",
    "    os.path.join(data_dir, \"..\", \"model_checkpoints\"))\n",
    "pkl_dir = os.path.join(output_dir, \"pkl\")\n",
    "stats_dir = os.path.join(output_dir, \"analysis\")\n",
    "\n",
    "# Create analysis dir if it doesn't exist\n",
    "os.makedirs(stats_dir, exist_ok=True)\n",
    "\n",
    "stats_filename = TARGET + \"_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "with open(os.path.join(pkl_dir, TARGET + \"_trimmed_seqs.pkl\"), \"rb\") as f:\n",
    "    inputs = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
    "    vocab = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"feature_lookup.pkl\"), \"rb\") as f:\n",
    "    all_feats = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
    "    demog_lookup = pkl.load(f)\n",
    "\n",
    "# Determining number of vocab entries\n",
    "N_VOCAB = len(vocab) + 1\n",
    "N_DEMOG = len(demog_lookup) + 1\n",
    "MAX_DEMOG = max(max(x) for _, x, _ in inputs)\n",
    "N_CLASS = max(x for _, _, x in inputs) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacked from https://github.com/Tony607/Focal_Loss_Keras/blob/master/src/keras_focal_loss.ipynb\n",
    "class FocalLoss(keras.losses.Loss):\n",
    "    def __init__(self, gamma=2., alpha=4.,\n",
    "                 reduction=keras.losses.Reduction.AUTO, name='focal_loss'):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__(reduction=reduction,\n",
    "                                        name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(\n",
    "            tf.subtract(1., model_out), self.gamma))\n",
    "        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        \n",
    "        return tf.reduce_mean(reduced_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics and callbacks\n",
    "callbacks = [\n",
    "    TensorBoard(\n",
    "    log_dir=os.path.join(tensorboard_dir, \"new_lstm_topo\", \"\"),\n",
    "    histogram_freq=1,\n",
    "    profile_batch=0,\n",
    "    write_graph=False,\n",
    "    update_freq=TB_UPDATE_FREQ\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                min_delta=0,\n",
    "                                patience=3,\n",
    "                                restore_best_weights=True,\n",
    "                                mode=\"auto\")\n",
    "]\n",
    "\n",
    "# Create some metrics\n",
    "metrics = [\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), name=\"ROC-AUC\"),\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), curve=\"PR\", name=\"PR-AUC\"),\n",
    "]\n",
    "\n",
    "# Define loss function\n",
    "# NOTE: We were experimenting with focal loss at one point, maybe we can try that again at some point\n",
    "# loss_fn = keras.losses.categorical_crossentropy if TARGET == \"multi_class\" else keras.losses.binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTV\n",
    "# Splitting the data\n",
    "train, test = train_test_split(\n",
    "    range(len(inputs)),\n",
    "    test_size=TEST_SPLIT,\n",
    "    stratify=[labs for _, _, labs in inputs],\n",
    "    random_state=RAND)\n",
    "\n",
    "train, validation = train_test_split(\n",
    "    train,\n",
    "    test_size=VAL_SPLIT,\n",
    "    stratify=[samp[2] for i, samp in enumerate(inputs) if i in train],\n",
    "    random_state=RAND)\n",
    "\n",
    "train_gen = tk.create_ragged_data_gen(\n",
    "    [inputs[samp] for samp in train],\n",
    "    max_demog=MAX_DEMOG,\n",
    "    epochs=EPOCHS,\n",
    "    multiclass=N_CLASS > 2,\n",
    "    random_seed=RAND,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_gen = tk.create_ragged_data_gen(\n",
    "    [inputs[samp] for samp in validation],\n",
    "    max_demog=MAX_DEMOG,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=False,\n",
    "    multiclass=N_CLASS > 2,\n",
    "    random_seed=RAND,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# NOTE: don't shuffle test data\n",
    "test_gen = tk.create_ragged_data_gen([inputs[samp] for samp in test],\n",
    "                                        max_demog=MAX_DEMOG,\n",
    "                                        epochs=1,\n",
    "                                        multiclass=N_CLASS > 2,\n",
    "                                        shuffle=False,\n",
    "                                        random_seed=RAND,\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "\n",
    "# %% Compute steps-per-epoch\n",
    "# NOTE: Sometimes it can't determine this properly from tf.data\n",
    "STEPS_PER_EPOCH = np.ceil(len(train) / BATCH_SIZE)\n",
    "VALID_STEPS_PER_EPOCH = np.ceil(len(validation) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{0: 3.530688622754491, 1: 1.4372333942717854, 2: 0.4948075107521242}\n"
    }
   ],
   "source": [
    "classes = np.unique([labs for _, _, labs in inputs]).tolist()\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=[labs for _, _, labs in inputs],\n",
    ")\n",
    "\n",
    "class_weights = dict(zip(classes, class_weights))\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Model builder\n",
    "class LSTMHyper(kerastuner.HyperModel):\n",
    "    def __init__(self, vocab_size, metrics, loss = None, n_classes=1, n_demog=32):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_demog = n_demog\n",
    "        self.metrics = metrics,\n",
    "        self.loss = loss\n",
    "\n",
    "    def build(self, hp: kerastuner.HyperParameters) -> keras.Model:\n",
    "\n",
    "        # L1/L2 vals\n",
    "        reg_vals = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "        # Model Topology\n",
    "\n",
    "        # Should we use a \"conditional RNN\"\n",
    "        rnn_conditional = hp.Boolean(\"Conditional RNN\")\n",
    "\n",
    "        # Should we multiply the feature embeddings by their averages?\n",
    "        weighting = hp.Boolean(\"Feature Weighting\")\n",
    "\n",
    "        # Should we add a dense layer between RNN and output?\n",
    "        final_dense = hp.Boolean(\"Final Dense Layer\")\n",
    "\n",
    "        # Feature Embedding Params\n",
    "        emb_l1 = hp.Choice(\"Feature Embedding L1\", reg_vals)\n",
    "        emb_l2 = hp.Choice(\"Feature Embedding L2\", reg_vals)\n",
    "\n",
    "        emb_n = hp.Int(\"Embedding Dimension\",\n",
    "                                    min_value=64,\n",
    "                                    max_value=512,\n",
    "                                    default=64,\n",
    "                                    step=64)\n",
    "\n",
    "        # Average Embedding Params\n",
    "        avg_l1 = hp.Choice(\"Average Embedding L1\", reg_vals,\n",
    "                                    parent_name = \"Feature Weighting\",\n",
    "                                    parent_values = [True])\n",
    "        avg_l2 = hp.Choice(\"Average Embedding L2\", reg_vals,\n",
    "                                    parent_name = \"Feature Weighting\",\n",
    "                                    parent_values = [True])\n",
    "\n",
    "        # LSTM Params\n",
    "        lstm_n = hp.Int(\"LSTM Units\",\n",
    "                        min_value=32,\n",
    "                        max_value=512,\n",
    "                        default=32,\n",
    "                        step=32)\n",
    "        lstm_dropout = hp.Float(\"LSTM Dropout\",\n",
    "                                min_value=0.0,\n",
    "                                max_value=0.9,\n",
    "                                default=0.4,\n",
    "                                step=0.01)\n",
    "        lstm_recurrent_dropout = hp.Float(\"LSTM Recurrent Dropout\",\n",
    "                                            min_value=0.0,\n",
    "                                            max_value=0.9,\n",
    "                                            default=0.4,\n",
    "                                            step=0.01)\n",
    "        lstm_l1 = hp.Choice(\"LSTM weights L1\", reg_vals)\n",
    "        lstm_l2 = hp.Choice(\"LSTM weights L2\", reg_vals)\n",
    "        \n",
    "        # Final dense layer\n",
    "        dense_n = hp.Int(\"Dense Units\",\n",
    "                         min_value=2,\n",
    "                         max_value=128,\n",
    "                         sampling=\"log\",\n",
    "                         parent_name=\"Final Dense Layer\",\n",
    "                         parent_values=[True]\n",
    "                         )\n",
    "        # Model code\n",
    "        feat_input = keras.Input(shape=(None, None), ragged=True)\n",
    "        demog_input = keras.Input(shape=(self.n_demog, ))\n",
    "\n",
    "        emb1 = keras.layers.Embedding(self.vocab_size,\n",
    "                                    output_dim=emb_n,\n",
    "                                    embeddings_regularizer=keras.regularizers.l1_l2(emb_l1, emb_l2),\n",
    "                                    mask_zero=True,\n",
    "                                    name=\"Feature_Embeddings\")(feat_input)\n",
    "        \n",
    "        if weighting:\n",
    "            emb2 = keras.layers.Embedding(self.vocab_size,\n",
    "                                          output_dim=1,\n",
    "                                          embeddings_regularizer=keras.regularizers.l1_l2(avg_l1, avg_l2),\n",
    "                                          mask_zero=True,\n",
    "                                          name=\"Average_Embeddings\")(feat_input)\n",
    "\n",
    "            # Multiplying the code embeddings by their respective weights\n",
    "            mult = keras.layers.Multiply(name=\"Embeddings_by_Average\")([emb1, emb2])\n",
    "            avg = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=2), name=\"Averaging\")(mult)\n",
    "        else:\n",
    "            avg = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=2), name=\"Averaging\")(emb1)\n",
    "        \n",
    "        if rnn_conditional:\n",
    "            lstm_layer = cond_rnn.ConditionalRNN(lstm_n, \n",
    "                                   dropout=lstm_dropout,\n",
    "                                   recurrent_dropout=lstm_recurrent_dropout,\n",
    "                                   kernel_regularizer=keras.regularizers.l1_l2(lstm_l1, lstm_l2),\n",
    "                                   name=\"Recurrent\")([avg, demog_input])\n",
    "\n",
    "        else:\n",
    "            lstm_layer = keras.layers.LSTM(lstm_n, \n",
    "                                   dropout=lstm_dropout,\n",
    "                                   recurrent_dropout=lstm_recurrent_dropout,\n",
    "                                   kernel_regularizer=keras.regularizers.l1_l2(lstm_l1, lstm_l2),\n",
    "                                   name=\"Recurrent\")(avg)\n",
    "        \n",
    "            demog_avg = keras.layers.Dense(lstm_n)(demog_input)\n",
    "            lstm_layer = keras.layers.Concatenate()([lstm_layer, demog_avg])\n",
    "\n",
    "        if final_dense:\n",
    "            lstm_layer = keras.layers.Dense(dense_n, activation = \"relu\", name = \"pre_output\")(lstm_layer)\n",
    "\n",
    "        output = keras.layers.Dense(\n",
    "            self.n_classes if self.n_classes > 2 else 1,\n",
    "            activation=\"sigmoid\",\n",
    "            name=\"Output\")(lstm_layer)\n",
    "\n",
    "        model = keras.Model([feat_input, demog_input], output)\n",
    "\n",
    "        # --- Focal Loss Hyperparameters\n",
    "        # Note: For gamma=0, focal loss is identical to crossentropy\n",
    "        hyper_gamma = hp.Choice(\"Focal gamma\", [0., 0.1, 0.2, 0.5, 1.0, 2.0, 5.0])\n",
    "        hyper_alpha = hp.Choice(\"Focal alpha\", [.1, .25, .5, .75, .9, .99, .999, 1.0, 2.0])\n",
    "\n",
    "        # --- Learning rate and momentum\n",
    "        lr = hp.Choice(\"Learning Rate\", [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1])\n",
    "        momentum = hp.Float(\"Momentum\", min_value=0.0, max_value=0.9, step=0.1)\n",
    "\n",
    "        model.compile(optimizer = keras.optimizers.SGD(lr, momentum=momentum), loss=FocalLoss(gamma=hyper_gamma, alpha=hyper_alpha, name=\"loss\"), metrics=self.metrics)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:Layer Recurrent will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
    }
   ],
   "source": [
    "hyper_model = LSTMHyper(\n",
    "    vocab_size = N_VOCAB,\n",
    "    metrics = metrics,\n",
    "    n_classes = N_CLASS,\n",
    "    n_demog = MAX_DEMOG\n",
    ")\n",
    "\n",
    "tuner = kerastuner.tuners.Hyperband(\n",
    "    hyper_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=EPOCHS,\n",
    "    hyperband_iterations=5,\n",
    "    project_name=\"new_lstm_topo\",\n",
    "    # NOTE: This could be in output as well if we don't want to track/version it\n",
    "    directory=tensorboard_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Search space summary\nDefault search space size: 18\nConditional RNN (Boolean)\n{'default': False, 'conditions': []}\nFeature Weighting (Boolean)\n{'default': False, 'conditions': []}\nFinal Dense Layer (Boolean)\n{'default': False, 'conditions': []}\nFeature Embedding L1 (Choice)\n{'default': 0.0, 'conditions': [], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\nFeature Embedding L2 (Choice)\n{'default': 0.0, 'conditions': [], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\nEmbedding Dimension (Int)\n{'default': 64, 'conditions': [], 'min_value': 64, 'max_value': 512, 'step': 64, 'sampling': None}\nAverage Embedding L1 (Choice)\n{'default': 0.0, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'Feature Weighting', 'values': [1]}}], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\nAverage Embedding L2 (Choice)\n{'default': 0.0, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'Feature Weighting', 'values': [1]}}], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\nLSTM Units (Int)\n{'default': 32, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\nLSTM Dropout (Float)\n{'default': 0.4, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.01, 'sampling': None}\nLSTM Recurrent Dropout (Float)\n{'default': 0.4, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.01, 'sampling': None}\nLSTM weights L1 (Choice)\n{'default': 0.0, 'conditions': [], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\nLSTM weights L2 (Choice)\n{'default': 0.0, 'conditions': [], 'values': [0.0, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1], 'ordered': True}\nDense Units (Int)\n{'default': None, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'Final Dense Layer', 'values': [1]}}], 'min_value': 2, 'max_value': 128, 'step': 1, 'sampling': 'log'}\nFocal gamma (Choice)\n{'default': 0.0, 'conditions': [], 'values': [0.0, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0], 'ordered': True}\nFocal alpha (Choice)\n{'default': 0.1, 'conditions': [], 'values': [0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 0.999, 1.0, 2.0], 'ordered': True}\nLearning Rate (Choice)\n{'default': 1e-06, 'conditions': [], 'values': [1e-06, 5e-06, 1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1], 'ordered': True}\nMomentum (Float)\n{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.1, 'sampling': None}\n"
    }
   ],
   "source": [
    "# Announce the search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_gen,\n",
    "             validation_data=validation_gen,\n",
    "             epochs=EPOCHS,\n",
    "             steps_per_epoch=STEPS_PER_EPOCH,\n",
    "             validation_steps=VALID_STEPS_PER_EPOCH,\n",
    "             callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(os.path.join(tensorboard_dir, \"new_lstm_topo\", \"best\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "name": "python38264bitvenvvenv11f8ace3faad4dd08d79c36b9c39aaa4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}