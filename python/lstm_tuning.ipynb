{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\r\n",
    "import csv\r\n",
    "import os\r\n",
    "import pickle as pkl\r\n",
    "import pandas as pd\r\n",
    "import argparse\r\n",
    "import cond_rnn\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow.keras as keras\r\n",
    "import kerastuner\r\n",
    "from kerastuner import HyperModel\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from tensorflow.keras.callbacks import TensorBoard\r\n",
    "\r\n",
    "import tools.analysis as ta\r\n",
    "from tools import keras as tk\r\n",
    "import tools.preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # GLOBALS   \r\n",
    "TIME_SEQ = 225\r\n",
    "TARGET = \"multi_class\"\r\n",
    "BATCH_SIZE = 32\r\n",
    "EPOCHS = 15\r\n",
    "TEST_SPLIT = 0.2\r\n",
    "VAL_SPLIT = 0.1\r\n",
    "RAND = 2021\r\n",
    "TB_UPDATE_FREQ = 200\r\n",
    "\r\n",
    "# Paths\r\n",
    "output_dir = os.path.abspath(os.path.join(\"..\", \"output\"))\r\n",
    "data_dir = os.path.abspath(os.path.join(\"..\", \"data\", \"data\"))\r\n",
    "tensorboard_dir = os.path.abspath(\r\n",
    "    os.path.join(data_dir, \"..\", \"model_checkpoints\"))\r\n",
    "pkl_dir = os.path.join(output_dir, \"pkl\")\r\n",
    "stats_dir = os.path.join(output_dir, \"analysis\")\r\n",
    "\r\n",
    "# Create analysis dir if it doesn't exist\r\n",
    "os.makedirs(stats_dir, exist_ok=True)\r\n",
    "\r\n",
    "stats_filename = TARGET + \"_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\r\n",
    "with open(os.path.join(pkl_dir, TARGET + \"_trimmed_seqs.pkl\"), \"rb\") as f:\r\n",
    "    inputs = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\r\n",
    "    vocab = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"feature_lookup.pkl\"), \"rb\") as f:\r\n",
    "    all_feats = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\r\n",
    "    demog_lookup = pkl.load(f)\r\n",
    "\r\n",
    "# Determining number of vocab entries\r\n",
    "N_VOCAB = len(vocab) + 1\r\n",
    "N_DEMOG = len(demog_lookup) + 1\r\n",
    "MAX_DEMOG = max(max(x) for _, x, _ in inputs)\r\n",
    "N_CLASS = max(x for _, _, x in inputs) + 1\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics and callbacks\r\n",
    "callbacks = [\r\n",
    "    TensorBoard(\r\n",
    "    log_dir=os.path.join(tensorboard_dir, \"new_lstm_topo\", \"\"),\r\n",
    "    histogram_freq=1,\r\n",
    "    update_freq=TB_UPDATE_FREQ\r\n",
    "    ),\r\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\r\n",
    "                                min_delta=0,\r\n",
    "                                patience=3,\r\n",
    "                                restore_best_weights=True,\r\n",
    "                                mode=\"auto\")\r\n",
    "]\r\n",
    "\r\n",
    "# Create some metrics\r\n",
    "metrics = [\r\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), name=\"ROC-AUC\"),\r\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), curve=\"PR\", name=\"PR-AUC\"),\r\n",
    "]\r\n",
    "\r\n",
    "# Define loss function\r\n",
    "# NOTE: We were experimenting with focal loss at one point, maybe we can try that again at some point\r\n",
    "loss_fn = keras.losses.categorical_crossentropy if TARGET == \"multi_class\" else keras.losses.binary_crossentropy\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10188, 11)\n",
      "(1132, 11)\n",
      "(2831, 11)\n"
     ]
    }
   ],
   "source": [
    "# TTV\r\n",
    "# Splitting the data\r\n",
    "train, test = train_test_split(\r\n",
    "    range(len(inputs)),\r\n",
    "    test_size=TEST_SPLIT,\r\n",
    "    stratify=[labs for _, _, labs in inputs],\r\n",
    "    random_state=RAND)\r\n",
    "\r\n",
    "train, validation = train_test_split(\r\n",
    "    train,\r\n",
    "    test_size=VAL_SPLIT,\r\n",
    "    stratify=[samp[2] for i, samp in enumerate(inputs) if i in train],\r\n",
    "    random_state=RAND)\r\n",
    "\r\n",
    "train_gen = tk.create_ragged_data_gen(\r\n",
    "    [inputs[samp] for samp in train],\r\n",
    "    max_demog=MAX_DEMOG,\r\n",
    "    epochs=EPOCHS,\r\n",
    "    multiclass=N_CLASS > 2,\r\n",
    "    random_seed=RAND,\r\n",
    "    batch_size=BATCH_SIZE)\r\n",
    "\r\n",
    "validation_gen = tk.create_ragged_data_gen(\r\n",
    "    [inputs[samp] for samp in validation],\r\n",
    "    max_demog=MAX_DEMOG,\r\n",
    "    epochs=EPOCHS,\r\n",
    "    shuffle=False,\r\n",
    "    multiclass=N_CLASS > 2,\r\n",
    "    random_seed=RAND,\r\n",
    "    batch_size=BATCH_SIZE)\r\n",
    "\r\n",
    "# NOTE: don't shuffle test data\r\n",
    "test_gen = tk.create_ragged_data_gen([inputs[samp] for samp in test],\r\n",
    "                                        max_demog=MAX_DEMOG,\r\n",
    "                                        epochs=1,\r\n",
    "                                        multiclass=N_CLASS > 2,\r\n",
    "                                        shuffle=False,\r\n",
    "                                        random_seed=RAND,\r\n",
    "                                        batch_size=BATCH_SIZE)\r\n",
    "\r\n",
    "# %% Compute steps-per-epoch\r\n",
    "# NOTE: Sometimes it can't determine this properly from tf.data\r\n",
    "STEPS_PER_EPOCH = np.ceil(len(train) / BATCH_SIZE)\r\n",
    "VALID_STEPS_PER_EPOCH = np.ceil(len(validation) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Model builder\r\n",
    "class LSTMHyper(kerastuner.HyperModel):\r\n",
    "    def __init__(self, vocab_size, metrics, loss, n_classes=1, n_demog=32):\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.n_classes = n_classes\r\n",
    "        self.n_demog = n_demog\r\n",
    "        self.metrics = metrics,\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "    def build(self, hp: kerastuner.HyperParameters) -> keras.Model:\r\n",
    "        # Model Topology\r\n",
    "\r\n",
    "        # Should we use a \"conditional RNN\"\r\n",
    "        rnn_conditional = hp.Boolean(\"Conditional RNN\")\r\n",
    "\r\n",
    "        # Should we multiply the feature embeddings by their averages?\r\n",
    "        weighting = hp.Boolean(\"Feature Weighting\")\r\n",
    "\r\n",
    "        # Should we add a dense layer between RNN and output?\r\n",
    "        final_dense = hp.Boolean(\"Final Dense Layer\")\r\n",
    "\r\n",
    "        # Feature Embedding Params\r\n",
    "        emb_l1 = hp.Float(\"Feature Embedding L1\",\r\n",
    "                                    min_value=0.0,\r\n",
    "                                    max_value=0.1,\r\n",
    "                                    step=0.01)\r\n",
    "        emb_l2 = hp.Float(\"Feature Embedding L2\",\r\n",
    "                                    min_value=0.0,\r\n",
    "                                    max_value=0.1,\r\n",
    "                                    step=0.01)\r\n",
    "        emb_n = hp.Int(\"Embedding Dimension\",\r\n",
    "                                    min_value=64,\r\n",
    "                                    max_value=512,\r\n",
    "                                    default=64,\r\n",
    "                                    step=64)\r\n",
    "\r\n",
    "        # Average Embedding Params\r\n",
    "        avg_l1 = hp.Float(\"Average Embedding L1\",\r\n",
    "                                    min_value=0.0,\r\n",
    "                                    max_value=0.1,\r\n",
    "                                    step=0.01,\r\n",
    "                                    parent_name = \"Feature Weighting\",\r\n",
    "                                    parent_values = [True])\r\n",
    "        avg_l2 = hp.Float(\"Average Embedding L2\",\r\n",
    "                                    min_value=0.0,\r\n",
    "                                    max_value=0.1,\r\n",
    "                                    step=0.01,\r\n",
    "                                    parent_name = \"Feature Weighting\",\r\n",
    "                                    parent_values = [True])\r\n",
    "\r\n",
    "        # LSTM Params\r\n",
    "        lstm_n = hp.Int(\"LSTM Units\",\r\n",
    "                        min_value=32,\r\n",
    "                        max_value=512,\r\n",
    "                        default=32,\r\n",
    "                        step=32)\r\n",
    "        lstm_dropout = hp.Float(\"LSTM Dropout\",\r\n",
    "                                min_value=0.0,\r\n",
    "                                max_value=0.9,\r\n",
    "                                default=0.4,\r\n",
    "                                step=0.01)\r\n",
    "        lstm_recurrent_dropout = hp.Float(\"LSTM Recurrent Dropout\",\r\n",
    "                                            min_value=0.0,\r\n",
    "                                            max_value=0.9,\r\n",
    "                                            default=0.4,\r\n",
    "                                            step=0.01)\r\n",
    "        lstm_l1 = hp.Float(\"LSTM weights L1\",\r\n",
    "                            min_value=0.0,\r\n",
    "                            max_value=0.1,\r\n",
    "                            step=0.01)\r\n",
    "        lstm_l2 = hp.Float(\"LSTM weights L2\",\r\n",
    "                            min_value=0.0,\r\n",
    "                            max_value=0.1,\r\n",
    "                            step=0.01)\r\n",
    "        \r\n",
    "        # Final dense layer\r\n",
    "        dense_n = hp.Int(\"Dense Units\",\r\n",
    "                         min_value=2,\r\n",
    "                         max_value=128,\r\n",
    "                         sampling=\"log\",\r\n",
    "                         parent_name=\"Final Dense Layer\",\r\n",
    "                         parent_values=[True]\r\n",
    "                         )\r\n",
    "        # Model code\r\n",
    "        feat_input = keras.Input(shape=(None, None), ragged=True)\r\n",
    "        demog_input = keras.Input(shape=(self.n_demog, ))\r\n",
    "\r\n",
    "        emb1 = keras.layers.Embedding(self.vocab_size,\r\n",
    "                                    output_dim=emb_n,\r\n",
    "                                    embeddings_regularizer=keras.regularizers.l1_l2(emb_l1, emb_l2),\r\n",
    "                                    mask_zero=True,\r\n",
    "                                    name=\"Feature_Embeddings\")(feat_input)\r\n",
    "        \r\n",
    "        if weighting:\r\n",
    "            emb2 = keras.layers.Embedding(self.vocab_size,\r\n",
    "                                          output_dim=1,\r\n",
    "                                          embeddings_regularizer=keras.regularizers.l1_l2(avg_l1, avg_l2),\r\n",
    "                                          mask_zero=True,\r\n",
    "                                          name=\"Average_Embeddings\")(feat_input)\r\n",
    "\r\n",
    "            # Multiplying the code embeddings by their respective weights\r\n",
    "            mult = keras.layers.Multiply(name=\"Embeddings_by_Average\")([emb1, emb2])\r\n",
    "            avg = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=2), name=\"Averaging\")(mult)\r\n",
    "        else:\r\n",
    "            avg = keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=2), name=\"Averaging\")(emb1)\r\n",
    "        \r\n",
    "        if rnn_conditional:\r\n",
    "            lstm_layer = cond_rnn.ConditionalRNN(lstm_n, \r\n",
    "                                   dropout=lstm_dropout,\r\n",
    "                                   recurrent_dropout=lstm_recurrent_dropout,\r\n",
    "                                   kernel_regularizer=keras.regularizers.l1_l2(lstm_l1, lstm_l2),\r\n",
    "                                   name=\"Recurrent\")([avg, demog_input])\r\n",
    "\r\n",
    "        else:\r\n",
    "            lstm_layer = keras.layers.LSTM(lstm_n, \r\n",
    "                                   dropout=lstm_dropout,\r\n",
    "                                   recurrent_dropout=lstm_recurrent_dropout,\r\n",
    "                                   kernel_regularizer=keras.regularizers.l1_l2(lstm_l1, lstm_l2),\r\n",
    "                                   name=\"Recurrent\")(avg)\r\n",
    "        \r\n",
    "            demog_avg = keras.layers.Dense(lstm_n)(demog_input)\r\n",
    "            lstm_layer = keras.layers.Concatenate()([lstm_layer, demog_avg])\r\n",
    "\r\n",
    "        if final_dense:\r\n",
    "            lstm_layer = keras.layers.Dense(dense_n, activation = \"relu\", name = \"pre_output\")(lstm_layer)\r\n",
    "\r\n",
    "        output = keras.layers.Dense(\r\n",
    "            self.n_classes if self.n_classes > 2 else 1,\r\n",
    "            activation=\"sigmoid\",\r\n",
    "            name=\"Output\")(lstm_layer)\r\n",
    "\r\n",
    "        model = keras.Model([feat_input, demog_input], output)\r\n",
    "\r\n",
    "        model.compile(optimizer = \"adam\", loss=self.loss, metrics=self.metrics)\r\n",
    "\r\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project data/model_checkpoints/new_lstm_topo\\oracle.json\n",
      "WARNING:tensorflow:Layer Recurrent will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "INFO:tensorflow:Reloading Tuner from data/model_checkpoints/new_lstm_topo\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "hyper_model = LSTMHyper(\r\n",
    "    vocab_size = N_VOCAB,\r\n",
    "    metrics = metrics,\r\n",
    "    loss = loss_fn,\r\n",
    "    n_classes = N_CLASS,\r\n",
    "    n_demog = MAX_DEMOG\r\n",
    ")\r\n",
    "\r\n",
    "tuner = kerastuner.tuners.Hyperband(\r\n",
    "    hyper_model,\r\n",
    "    objective=\"val_loss\",\r\n",
    "    max_epochs=EPOCHS,\r\n",
    "    hyperband_iterations=5,\r\n",
    "    project_name=\"new_lstm_topo\",\r\n",
    "    # NOTE: This could be in output as well if we don't want to track/version it\r\n",
    "    directory=tensorboard_dir,\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 14\n",
      "Conditional RNN (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "Feature Weighting (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "Final Dense Layer (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "Feature Embedding L1 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': None}\n",
      "Feature Embedding L2 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': None}\n",
      "Embedding Dimension (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 64, 'max_value': 512, 'step': 64, 'sampling': None}\n",
      "Average Embedding L1 (Float)\n",
      "{'default': 0.0, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'Feature Weighting', 'values': [1]}}], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': None}\n",
      "Average Embedding L2 (Float)\n",
      "{'default': 0.0, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'Feature Weighting', 'values': [1]}}], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': None}\n",
      "LSTM Units (Int)\n",
      "{'default': 32, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "LSTM Dropout (Float)\n",
      "{'default': 0.4, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.01, 'sampling': None}\n",
      "LSTM Recurrent Dropout (Float)\n",
      "{'default': 0.4, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.01, 'sampling': None}\n",
      "LSTM weights L1 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': None}\n",
      "LSTM weights L2 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.1, 'step': 0.01, 'sampling': None}\n",
      "Dense Units (Int)\n",
      "{'default': None, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'Final Dense Layer', 'values': [1]}}], 'min_value': 2, 'max_value': 128, 'step': 1, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "# Announce the search space\r\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 149 Complete [00h 04m 56s]\n",
      "val_loss: 18.912052154541016\n",
      "\n",
      "Best val_loss So Far: 1.2175601720809937\n",
      "Total elapsed time: 10h 25m 24s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_gen,\r\n",
    "             validation_data=validation_gen,\r\n",
    "             epochs=EPOCHS,\r\n",
    "             steps_per_epoch=STEPS_PER_EPOCH,\r\n",
    "             validation_steps=VALID_STEPS_PER_EPOCH,\r\n",
    "             callbacks=callbacks\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in data/model_checkpoints/new_lstm_topo\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.0\n",
      "Feature Embedding L2: 0.0\n",
      "Embedding Dimension: 448\n",
      "LSTM Units: 64\n",
      "LSTM Dropout: 0.44\n",
      "LSTM Recurrent Dropout: 0.38\n",
      "LSTM weights L1: 0.03\n",
      "LSTM weights L2: 0.09\n",
      "tuner/epochs: 15\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: aea680825de0164cbcc05e038c91dfbd\n",
      "Score: 1.2175601720809937\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.0\n",
      "Feature Embedding L2: 0.0\n",
      "Embedding Dimension: 448\n",
      "LSTM Units: 64\n",
      "LSTM Dropout: 0.44\n",
      "LSTM Recurrent Dropout: 0.38\n",
      "LSTM weights L1: 0.03\n",
      "LSTM weights L2: 0.09\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 962aad15801ce5e55cc60b4710c7299b\n",
      "Score: 1.222415566444397\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.0\n",
      "Feature Embedding L2: 0.0\n",
      "Embedding Dimension: 448\n",
      "LSTM Units: 64\n",
      "LSTM Dropout: 0.44\n",
      "LSTM Recurrent Dropout: 0.38\n",
      "LSTM weights L1: 0.03\n",
      "LSTM weights L2: 0.09\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 1.2234572172164917\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.01\n",
      "Feature Embedding L2: 0.02\n",
      "Embedding Dimension: 64\n",
      "LSTM Units: 320\n",
      "LSTM Dropout: 0.55\n",
      "LSTM Recurrent Dropout: 0.36\n",
      "LSTM weights L1: 0.1\n",
      "LSTM weights L2: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 77c5088ed0bb2bc10a43900aa4f7c9c6\n",
      "Score: 2.97139835357666\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.01\n",
      "Feature Embedding L2: 0.02\n",
      "Embedding Dimension: 64\n",
      "LSTM Units: 320\n",
      "LSTM Dropout: 0.55\n",
      "LSTM Recurrent Dropout: 0.36\n",
      "LSTM weights L1: 0.1\n",
      "LSTM weights L2: 0.01\n",
      "tuner/epochs: 15\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 470eeae7cf53ebb25ca22729cbab2773\n",
      "Score: 2.980238199234009\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.01\n",
      "Feature Embedding L2: 0.02\n",
      "Embedding Dimension: 64\n",
      "LSTM Units: 320\n",
      "LSTM Dropout: 0.55\n",
      "LSTM Recurrent Dropout: 0.36\n",
      "LSTM weights L1: 0.1\n",
      "LSTM weights L2: 0.01\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 3.021285057067871\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: True\n",
      "Feature Embedding L1: 0.01\n",
      "Feature Embedding L2: 0.06\n",
      "Embedding Dimension: 128\n",
      "LSTM Units: 64\n",
      "LSTM Dropout: 0.38\n",
      "LSTM Recurrent Dropout: 0.14\n",
      "LSTM weights L1: 0.04\n",
      "LSTM weights L2: 0.02\n",
      "Dense Units: 2\n",
      "tuner/epochs: 15\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 3.504188060760498\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: True\n",
      "Feature Embedding L1: 0.01\n",
      "Feature Embedding L2: 0.08\n",
      "Embedding Dimension: 128\n",
      "LSTM Units: 224\n",
      "LSTM Dropout: 0.32\n",
      "LSTM Recurrent Dropout: 0.09\n",
      "LSTM weights L1: 0.02\n",
      "LSTM weights L2: 0.05\n",
      "Dense Units: 33\n",
      "tuner/epochs: 15\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 3e2d63a118cd889a7f8002f2c28f2cb1\n",
      "Score: 3.5074195861816406\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: True\n",
      "Feature Weighting: False\n",
      "Final Dense Layer: True\n",
      "Feature Embedding L1: 0.01\n",
      "Feature Embedding L2: 0.08\n",
      "Embedding Dimension: 128\n",
      "LSTM Units: 224\n",
      "LSTM Dropout: 0.32\n",
      "LSTM Recurrent Dropout: 0.09\n",
      "LSTM weights L1: 0.02\n",
      "LSTM weights L2: 0.05\n",
      "Dense Units: 33\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 3.5194897651672363\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Conditional RNN: False\n",
      "Feature Weighting: True\n",
      "Final Dense Layer: False\n",
      "Feature Embedding L1: 0.02\n",
      "Feature Embedding L2: 0.09\n",
      "Embedding Dimension: 64\n",
      "Average Embedding L1: 0.08\n",
      "Average Embedding L2: 0.03\n",
      "LSTM Units: 192\n",
      "LSTM Dropout: 0.73\n",
      "LSTM Recurrent Dropout: 0.16\n",
      "LSTM weights L1: 0.1\n",
      "LSTM weights L2: 0.01\n",
      "tuner/epochs: 15\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: c621fa06bc51be958bf44f06c46ab86a\n",
      "Score: 4.004547119140625\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(1, None, 11)]      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile (TensorFlowOpL [(2, None, 11)]      0           tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Feature_Embeddings (Embedding)  (None, None, None, 4 6716864     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (2, None, 64)        768         tf_op_layer_Tile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Averaging (Lambda)              (None, None, 448)    0           Feature_Embeddings[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack (TensorFlow [(None, 64), (None,  0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "rnn (RNN)                       (None, 64)           131328      Averaging[0][0]                  \n",
      "                                                                 tf_op_layer_unstack[0][0]        \n",
      "                                                                 tf_op_layer_unstack[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 3)            195         rnn[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 6,849,155\n",
      "Trainable params: 6,849,155\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Pull the best model\r\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\r\n",
    "best_model = tuner.hypermodel.build(best_hp)\r\n",
    "\r\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\blues\\work\\premier_analysis\\data\\model_checkpoints\\new_lstm_topo\\best\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(os.path.join(tensorboard_dir, \"new_lstm_topo\", \"best\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "name": "python386jvsc74a57bd089d31c033bf313603ee1de07f165bedecb9a1c2d7c2ff2b104ae0fae591794dd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}